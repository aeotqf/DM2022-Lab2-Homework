{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d80ae6d",
   "metadata": {},
   "source": [
    "Here I tried some tutorial I found on the Web for preprocessing to remove stopwords and perform stemming to then try train some models with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3be81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff1aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "199ee164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ellis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ellis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ellis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ellis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc05cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1639d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "training_data2 = pd.read_csv('data/training_data.csv',encoding='latin-1')\n",
    "test_data2 = pd.read_csv('data/test_data.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5635097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing blank rows\n",
    "training_data2['text'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b01570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all the text to lower case\n",
    "training_data2['text'] = [entry.lower() for entry in training_data2['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10d0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization \n",
    "training_data2['text']= [word_tokenize(entry) for entry in training_data2['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "583c2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just copypasted this function to try\n",
    "# to remove stop words and Word Stemming.\n",
    "# this function takes lots of time to finish\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(training_data2['text']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    training_data2.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbfd1495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[why, chester, ?, &lt;, lh, &gt;, &lt;, lh, &gt;]</td>\n",
       "      <td>['chester', 'lh', 'lh']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "      <td>train</td>\n",
       "      <td>[@, jaredleto, you, are, the, fish, that, jona...</td>\n",
       "      <td>['jaredleto', 'fish', 'jonah', 'except', 'fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[he, is, coming, back, again, and, gon, na, co...</td>\n",
       "      <td>['come', 'back', 'gon', 'na', 'come', 'quickly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[dei, is, really, such, a, beautiful, person, ...</td>\n",
       "      <td>['dei', 'really', 'beautiful', 'person', 'insi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[expressive, praise, is, also, an, expression,...</td>\n",
       "      <td>['expressive', 'praise', 'also', 'expression',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion identification  \\\n",
       "0  0x3140b1       sadness          train   \n",
       "1  0x368b73       disgust          train   \n",
       "2  0x296183  anticipation          train   \n",
       "3  0x2bd6e1           joy          train   \n",
       "4  0x2ee1dd  anticipation          train   \n",
       "\n",
       "                                                text  \\\n",
       "0              [why, chester, ?, <, lh, >, <, lh, >]   \n",
       "1  [@, jaredleto, you, are, the, fish, that, jona...   \n",
       "2  [he, is, coming, back, again, and, gon, na, co...   \n",
       "3  [dei, is, really, such, a, beautiful, person, ...   \n",
       "4  [expressive, praise, is, also, an, expression,...   \n",
       "\n",
       "                                          text_final  \n",
       "0                            ['chester', 'lh', 'lh']  \n",
       "1  ['jaredleto', 'fish', 'jonah', 'except', 'fuck...  \n",
       "2  ['come', 'back', 'gon', 'na', 'come', 'quickly...  \n",
       "3  ['dei', 'really', 'beautiful', 'person', 'insi...  \n",
       "4  ['expressive', 'praise', 'also', 'expression',...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the \"new\" training data with a preprocessed text_final embeding of words\n",
    "training_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "972a7565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[@, habbo, i, 've, seen, two, separate, colour...</td>\n",
       "      <td>['habbo', 'see', 'two', 'separate', 'colour', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[@, foxnews, @, kellyannepolls, no, serious, s...</td>\n",
       "      <td>['foxnews', 'kellyannepolls', 'serious', 'self...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[looking, for, a, new, car, ,, and, it, says, ...</td>\n",
       "      <td>['look', 'new', 'car', 'say', 'lady', 'owner',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[@, cineworld, âonly, the, braveâ, just, o...</td>\n",
       "      <td>['cineworld', 'fountain', 'park', 'showing', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[felt, like, total, dog, ð©, going, into, op...</td>\n",
       "      <td>['felt', 'like', 'total', 'dog', 'go', 'open',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id  emotion identification  \\\n",
       "0  0x28cc61      NaN           test   \n",
       "1  0x2db41f      NaN           test   \n",
       "2  0x2466f6      NaN           test   \n",
       "3  0x23f9e9      NaN           test   \n",
       "4  0x1fb4e1      NaN           test   \n",
       "\n",
       "                                                text  \\\n",
       "0  [@, habbo, i, 've, seen, two, separate, colour...   \n",
       "1  [@, foxnews, @, kellyannepolls, no, serious, s...   \n",
       "2  [looking, for, a, new, car, ,, and, it, says, ...   \n",
       "3  [@, cineworld, âonly, the, braveâ, just, o...   \n",
       "4  [felt, like, total, dog, ð©, going, into, op...   \n",
       "\n",
       "                                          text_final  \n",
       "0  ['habbo', 'see', 'two', 'separate', 'colour', ...  \n",
       "1  ['foxnews', 'kellyannepolls', 'serious', 'self...  \n",
       "2  ['look', 'new', 'car', 'say', 'lady', 'owner',...  \n",
       "3  ['cineworld', 'fountain', 'park', 'showing', '...  \n",
       "4  ['felt', 'like', 'total', 'dog', 'go', 'open',...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the \"new\" test data with a preprocessed text_final embeding of words\n",
    "test_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827cceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to dataframes the new data\n",
    "training_data2_df = pd.DataFrame(training_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "041e9a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[why, chester, ?, &lt;, lh, &gt;, &lt;, lh, &gt;]</td>\n",
       "      <td>['chester', 'lh', 'lh']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "      <td>train</td>\n",
       "      <td>[@, jaredleto, you, are, the, fish, that, jona...</td>\n",
       "      <td>['jaredleto', 'fish', 'jonah', 'except', 'fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[he, is, coming, back, again, and, gon, na, co...</td>\n",
       "      <td>['come', 'back', 'gon', 'na', 'come', 'quickly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[dei, is, really, such, a, beautiful, person, ...</td>\n",
       "      <td>['dei', 'really', 'beautiful', 'person', 'insi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[expressive, praise, is, also, an, expression,...</td>\n",
       "      <td>['expressive', 'praise', 'also', 'expression',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion identification  \\\n",
       "0  0x3140b1       sadness          train   \n",
       "1  0x368b73       disgust          train   \n",
       "2  0x296183  anticipation          train   \n",
       "3  0x2bd6e1           joy          train   \n",
       "4  0x2ee1dd  anticipation          train   \n",
       "\n",
       "                                                text  \\\n",
       "0              [why, chester, ?, <, lh, >, <, lh, >]   \n",
       "1  [@, jaredleto, you, are, the, fish, that, jona...   \n",
       "2  [he, is, coming, back, again, and, gon, na, co...   \n",
       "3  [dei, is, really, such, a, beautiful, person, ...   \n",
       "4  [expressive, praise, is, also, an, expression,...   \n",
       "\n",
       "                                          text_final  \n",
       "0                            ['chester', 'lh', 'lh']  \n",
       "1  ['jaredleto', 'fish', 'jonah', 'except', 'fuck...  \n",
       "2  ['come', 'back', 'gon', 'na', 'come', 'quickly...  \n",
       "3  ['dei', 'really', 'beautiful', 'person', 'insi...  \n",
       "4  ['expressive', 'praise', 'also', 'expression',...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77cae76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to CSV the new data training\n",
    "training_data2.to_csv('data/training_data_prepro.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e966841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same for test data\n",
    "test_data2['text'].dropna(inplace=True)\n",
    "test_data2['text'] = [entry.lower() for entry in test_data2['text']]\n",
    "test_data2['text']= [word_tokenize(entry) for entry in test_data2['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58e2e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same for test data\n",
    "# to remove stop words and Word Stemming.\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(test_data2['text']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    test_data2.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80ee950a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[@, habbo, i, 've, seen, two, separate, colour...</td>\n",
       "      <td>['habbo', 'see', 'two', 'separate', 'colour', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[@, foxnews, @, kellyannepolls, no, serious, s...</td>\n",
       "      <td>['foxnews', 'kellyannepolls', 'serious', 'self...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[looking, for, a, new, car, ,, and, it, says, ...</td>\n",
       "      <td>['look', 'new', 'car', 'say', 'lady', 'owner',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[@, cineworld, âonly, the, braveâ, just, o...</td>\n",
       "      <td>['cineworld', 'fountain', 'park', 'showing', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[felt, like, total, dog, ð©, going, into, op...</td>\n",
       "      <td>['felt', 'like', 'total', 'dog', 'go', 'open',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id  emotion identification  \\\n",
       "0  0x28cc61      NaN           test   \n",
       "1  0x2db41f      NaN           test   \n",
       "2  0x2466f6      NaN           test   \n",
       "3  0x23f9e9      NaN           test   \n",
       "4  0x1fb4e1      NaN           test   \n",
       "\n",
       "                                                text  \\\n",
       "0  [@, habbo, i, 've, seen, two, separate, colour...   \n",
       "1  [@, foxnews, @, kellyannepolls, no, serious, s...   \n",
       "2  [looking, for, a, new, car, ,, and, it, says, ...   \n",
       "3  [@, cineworld, âonly, the, braveâ, just, o...   \n",
       "4  [felt, like, total, dog, ð©, going, into, op...   \n",
       "\n",
       "                                          text_final  \n",
       "0  ['habbo', 'see', 'two', 'separate', 'colour', ...  \n",
       "1  ['foxnews', 'kellyannepolls', 'serious', 'self...  \n",
       "2  ['look', 'new', 'car', 'say', 'lady', 'owner',...  \n",
       "3  ['cineworld', 'fountain', 'park', 'showing', '...  \n",
       "4  ['felt', 'like', 'total', 'dog', 'go', 'open',...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fffd7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2.to_csv('data/test_data_prepro.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02244cbb",
   "metadata": {},
   "source": [
    "With this new \"preprocess\" data I tried again Naive bayes and later the NN however the accuracy got worst in every scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b60e3",
   "metadata": {},
   "source": [
    "4) SVM - never finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298604b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train9, y_train9)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(X_test9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
